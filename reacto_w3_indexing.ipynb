{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO9V8k/6fXtGfte8tC4Z9F0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RuthNjeri6/LLM-llama-2-demo/blob/main/reacto_w3_indexing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indexing using LLama-2 on colab"
      ],
      "metadata": {
        "id": "wmdSlILpSFPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain==0.0.264 pypdf==3.15.1 torch accelerate==0.21.0 transformers==4.31.0 sentence_transformers==2.2.2 ctransformers==0.2.22 faiss-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMVGNNN7SF0z",
        "outputId": "a8078741-7987-4065-bba3-3da229316ca4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import JSONLoader, DirectoryLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "u-e-9ddXS2w7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = './first_80_papers'\n",
        "DB_FAISS_PATH = 'vectorstore/db_faiss'"
      ],
      "metadata": {
        "id": "JQvv3rqfTAiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vector database\n",
        "def create_vector_db():\n",
        "    try:\n",
        "        loader = DirectoryLoader(DATA_PATH, glob=\"./*.pdf\", loader_cls=PyPDFLoader, show_progress=True, use_multithreading=True)\n",
        "        #loader = DirectoryLoader(DATA_PATH, glob=\"./*.json\", loader_cls=JSONLoader, loader_kwargs = {'jq_schema':'.pages[]'}, show_progress=True, use_multithreading=True)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return\n",
        "\n",
        "    documents = loader.load()\n",
        "    print(f\"Loaded {len(documents)} documents\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cuda'})\n",
        "\n",
        "    db = FAISS.from_documents(texts, embeddings)\n",
        "    db.save_local(DB_FAISS_PATH)"
      ],
      "metadata": {
        "id": "eDsdeOpeT_5x"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_vector_db()"
      ],
      "metadata": {
        "id": "cLtRPkf3UQqm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e370f9f-26ec-48a1-ced1-92d299c19294"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 59%|█████▉    | 72/122 [00:40<00:42,  1.18it/s]WARNING:pypdf._reader:EOF marker not found\n",
            "100%|██████████| 122/122 [01:08<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 994 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.llms import CTransformers\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "6bphh9VKURj-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
        "\n",
        "custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "Only return the helpful answer below and nothing else.\n",
        "Helpful answer:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Y8nOzLkcUntK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_custom_prompt():\n",
        "    \"\"\"\n",
        "    Prompt template for QA retrieval for each vectorstore\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(template=custom_prompt_template,\n",
        "                            input_variables=['context', 'question'])\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "5I8D_sb1UvoZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Retrieval QA Chain\n",
        "def retrieval_qa_chain(llm, prompt, db):\n",
        "    qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                       chain_type='stuff',\n",
        "                                       retriever=db.as_retriever(search_kwargs={'k': 2}),\n",
        "                                       return_source_documents=True,\n",
        "                                       chain_type_kwargs={'prompt': prompt}\n",
        "                                       )\n",
        "    return qa_chain"
      ],
      "metadata": {
        "id": "OC6eguF0ZFh_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the model\n",
        "def load_llm():\n",
        "    # Load the locally downloaded model here\n",
        "    llm = CTransformers(\n",
        "        model = \"llama-2-7b-chat.ggmlv3.q8_0.bin\",\n",
        "        model_type=\"llama\",\n",
        "        max_new_tokens = 512,\n",
        "        temperature = 0.5\n",
        "    )\n",
        "    return llm"
      ],
      "metadata": {
        "id": "N5tUQCZuZK8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mFe6m1BHZoI0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}