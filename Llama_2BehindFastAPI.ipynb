{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOdaAwN4PwgrLD6MHlgI/Zn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RuthNjeri6/LLM-llama-2-demo/blob/main/Llama_2BehindFastAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hosting Llama 2 with Free GPU via Google Collab**"
      ],
      "metadata": {
        "id": "4YLwEHaevrSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before getting started, if running on Google Colab, check that the runtime is set to T4 GPU"
      ],
      "metadata": {
        "id": "8d60uX-jvqgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies\n",
        "- Requirements for running FastAPI Server\n",
        "- Requirements for creating a public model serving URL via Ngrok\n",
        "- Requirements for running Llama2 13B (including Quantization)\n"
      ],
      "metadata": {
        "id": "QBmoCdZyvy0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Llama cpp\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqGFsdSSv-rj",
        "outputId": "2511ed51-dc65-4489-a83c-7659a27c71dd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.7.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=5822244 sha256=ab7152dbcc9f2bed72343cb02e12ae0d2fc71f9ccb240fb42c43d31021e7ea83\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.1 llama-cpp-python-0.1.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If this complains about dependency resolver, it's safe to ignore\n",
        "!pip install fastapi[all] uvicorn python-multipart transformers pydantic tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAetG29wv_zI",
        "outputId": "7967c101-4ab4-4d55-b4e8-4cf7055d3827"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi[all]\n",
            "  Downloading fastapi-0.101.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi[all])\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from fastapi[all]) (4.7.1)\n",
            "Collecting email-validator>=2.0.0 (from fastapi[all])\n",
            "  Downloading email_validator-2.0.0.post2-py3-none-any.whl (31 kB)\n",
            "Collecting httpx>=0.23.0 (from fastapi[all])\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from fastapi[all]) (2.1.2)\n",
            "Requirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi[all]) (3.1.2)\n",
            "Collecting orjson>=3.2.1 (from fastapi[all])\n",
            "  Downloading orjson-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic-extra-types>=2.0.0 (from fastapi[all])\n",
            "  Downloading pydantic_extra_types-2.0.0-py3-none-any.whl (13 kB)\n",
            "Collecting pydantic-settings>=2.0.0 (from fastapi[all])\n",
            "  Downloading pydantic_settings-2.0.3-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from fastapi[all]) (6.0.1)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi[all])\n",
            "  Downloading ujson-5.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.14)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.33.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.1)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[all])\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email-validator>=2.0.0->fastapi[all]) (3.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi[all]) (2023.7.22)\n",
            "Collecting httpcore<0.18.0,>=0.15.0 (from httpx>=0.23.0->fastapi[all])\n",
            "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi[all]) (1.3.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.2->fastapi[all]) (2.1.3)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings>=2.0.0->fastapi[all])\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi[all]) (3.7.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Collecting httptools>=0.5.0 (from uvicorn)\n",
            "  Downloading httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn)\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn)\n",
            "  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi[all]) (1.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Installing collected packages: tokenizers, safetensors, websockets, uvloop, ujson, python-multipart, python-dotenv, orjson, httptools, h11, dnspython, watchfiles, uvicorn, starlette, huggingface-hub, httpcore, email-validator, transformers, pydantic-settings, pydantic-extra-types, httpx, fastapi\n",
            "Successfully installed dnspython-2.4.2 email-validator-2.0.0.post2 fastapi-0.101.1 h11-0.14.0 httpcore-0.17.3 httptools-0.6.0 httpx-0.24.1 huggingface-hub-0.16.4 orjson-3.9.5 pydantic-extra-types-2.0.0 pydantic-settings-2.0.3 python-dotenv-1.0.0 python-multipart-0.0.6 safetensors-0.3.3 starlette-0.27.0 tokenizers-0.13.3 transformers-4.32.0 ujson-5.8.0 uvicorn-0.23.2 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This downloads and sets up the Ngrok executable in the Google Colab instance\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -o ngrok-stable-linux-amd64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wCp9Go-wDxE",
        "outputId": "74edd931-bd3f-47bd-d659-e11c67bbb09c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-23 20:04:29--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 18.205.222.128, 54.161.241.46, 54.237.133.81, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|18.205.222.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  5.31MB/s    in 2.5s    \n",
            "\n",
            "2023-08-23 20:04:32 (5.31 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13921656/13921656]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ngrok is used to make the FastAPI server accessible via a public URL.\n",
        "\n",
        "Users are required to make a free account and provide their auth token to use Ngrok. The free version only allows 1 local tunnel and the auth token is used to track this usage limit."
      ],
      "metadata": {
        "id": "P3m3DL4XwX8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://dashboard.ngrok.com/signup\n",
        "!./ngrok authtoken 2UOlnIn6XpWMU527NhXMToiGUcG_6y3mtDRUGHAZpEGLWguES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYVZsoQI1dKh",
        "outputId": "6dc3f1f6-40da-4177-95a4-0da511b011c8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create FastAPI App\n",
        "This provides an API to the Llama 2 model. The model version can be changed in the code below as desired.\n",
        "\n",
        "For this demo we will use the 13 billion parameter version which is finetuned for instruction (chat) following.\n",
        "\n",
        "Despite the compression, it is still a more powerful model than the 7B variant."
      ],
      "metadata": {
        "id": "vrvVnODp12Ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from typing import Any\n",
        "\n",
        "from fastapi import FastAPI\n",
        "from fastapi import HTTPException\n",
        "from pydantic import BaseModel\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# GGML model required to fit Llama2-7B on a T4 GPU\n",
        "GENERATIVE_AI_MODEL_REPO = \"TheBloke/Llama-2-7B-chat-GGML\"\n",
        "GENERATIVE_AI_MODEL_FILE = \"llama-2-7b-chat.ggmlv3.q5_1.bin\"\n",
        "\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=GENERATIVE_AI_MODEL_REPO,\n",
        "    filename=GENERATIVE_AI_MODEL_FILE\n",
        ")\n",
        "\n",
        "llama2_model = Llama(\n",
        "    model_path=model_path,\n",
        "    n_gpu_layers=64,\n",
        "    n_ctx=2000\n",
        ")\n",
        "\n",
        "# Test an inference\n",
        "print(llama2_model(prompt=\"Hello \", max_tokens=1))\n",
        "\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "\n",
        "# This defines the data json format expected for the endpoint, change as needed\n",
        "class TextInput(BaseModel):\n",
        "    inputs: str\n",
        "    parameters: dict[str, Any] | None\n",
        "\n",
        "\n",
        "@app.get(\"/\")\n",
        "def status_gpu_check() -> dict[str, str]:\n",
        "    gpu_msg = \"Available\" if tf.test.is_gpu_available() else \"Unavailable\"\n",
        "    return {\n",
        "        \"status\": \"I am ALIVE!\",\n",
        "        \"gpu\": gpu_msg\n",
        "    }\n",
        "\n",
        "\n",
        "@app.post(\"/generate/\")\n",
        "async def generate_text(data: TextInput) -> dict[str, str]:\n",
        "    try:\n",
        "        params = data.parameters or {}\n",
        "        response = llama2_model(prompt=data.inputs, **params)\n",
        "        model_out = response['choices'][0]['text']\n",
        "        return {\"generated_text\": model_out}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZig-ttw2hik",
        "outputId": "2764d92f-7945-4996-f3b0-645c5d67a577"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start FastAPI Server\n",
        "The initial run will take a long time due to having to download the model and load it onto GPU.\n",
        "\n",
        "Note: interrupting the Google Colab runtime will send a SIGINT and stop the server."
      ],
      "metadata": {
        "id": "mqddIYua7f6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell finishes quickly because it just needs to start up the server\n",
        "# The server will start the model download and will take a while to start up\n",
        "# ~5 minutes\n",
        "!uvicorn app:app --host 0.0.0.0 --port 8000 > server.log 2>&1 &"
      ],
      "metadata": {
        "id": "MEM6AMCg7jcH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the logs at server.log to see progress.\n",
        "\n",
        "Wait until model is loaded and check with the next cell before moving on."
      ],
      "metadata": {
        "id": "KRJaplsT7zMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you see \"Failed to connect\", it's because the server is still starting up\n",
        "# Wait for the model to be downloaded and the server to fully start\n",
        "# Check the server.log file to see the status\n",
        "!curl localhost:8000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynYa_ldt7pu1",
        "outputId": "5197fbe3-d587-4af3-9625-69afb1b94e0a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"status\":\"I am ALIVE!\",\"gpu\":\"Available\"}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Ngrok to create a public URL for the FastAPI server.\n",
        "**IMPORTANT:** If you created an account via email, please verify your email or the next 2 cells won't work.\n",
        "\n",
        "If you signed up via Google or GitHub account, you're good to go."
      ],
      "metadata": {
        "id": "A4Re0X2376_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This starts Ngrok and creates the public URL\n",
        "from IPython import get_ipython\n",
        "get_ipython().system_raw('./ngrok http 8000 &')"
      ],
      "metadata": {
        "id": "YTlJThK38A01"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the URL generated by the next cell, it should report that the FastAPI server is alive and that GPU is available.\n",
        "\n",
        "To hit the model endpoint, simply add `/generate` to the URL"
      ],
      "metadata": {
        "id": "y_2EWbLq8Fg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the Public URL\n",
        "# If this doesn't work, make sure you verified your email\n",
        "# Then run the previous code cell and this one again\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW6hK_WB8EkM",
        "outputId": "628594b9-3924-4dc5-851d-ff5ae653d318"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://e4cd-34-143-174-213.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl --location --request POST 'http://e4cd-34-143-174-213.ngrok-free.app/generate' --header 'Content-Type: application/json' --data-raw '{\"inputs\": \"What is mypy used for?\",\"parameters\": {\"temperature\": 0.0,\"max_tokens\": 25}}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OaLedRqCBcp",
        "outputId": "17a2c064-96ef-43ae-8a19-fc1001d898df"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"generated_text\":\"\\n\\nMypy is a static type checker for Python that can be used to catch type-related errors at compile\"}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shutting Down\n",
        "To shut down the processes, run the following commands in a new cell:\n",
        "```\n",
        "!pkill uvicorn\n",
        "!pkill ngrok\n",
        "```"
      ],
      "metadata": {
        "id": "4cDeTIA3Bx7t"
      }
    }
  ]
}